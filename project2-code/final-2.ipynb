{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\"\"\"\n",
    "Function used in sort\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def takeFirst(list):\n",
    "    return int(list[0])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This is the function that used to write the result to a new CSV file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def writeCSV(resultList):\n",
    "    with open('result.csv', 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"Id\", \"Class\"])\n",
    "        for list in resultList:\n",
    "            csv_writer.writerow(list)\n",
    "        csv_file.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function is used to generate two features dictionary, username dictionary and location dictionary.\n",
    "It will remove some low frequency features according to the number given in the parameter\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def featureEng(data, locNumber, userNumber):\n",
    "    # read tsv file\n",
    "    csv.register_dialect('mydialect', delimiter='\\t', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    locDict = {}\n",
    "    userDict = {}\n",
    "    csvfile = open(data, \"r\")\n",
    "    reader = csv.reader(csvfile, 'mydialect')\n",
    "\n",
    "    # read tsv file line by line\n",
    "    for row in reader:\n",
    "        # replace special character\n",
    "        tweeteStr = row[2].replace(',', ' ').replace('/', ' ').replace('\\\\', ' ').replace(')', ' ').replace('.', ' ')\n",
    "\n",
    "        # use reagular expression to find all the location string(format @ location ) and user name string(format @username )\n",
    "        locationList = re.findall(r\"@ (.*)\", tweeteStr)\n",
    "        userList = re.findall(r\"@(.+?) \", tweeteStr)\n",
    "\n",
    "        # add them to the dictionary\n",
    "        if len(locationList) > 0:\n",
    "            locationList = locationList[0].split()\n",
    "            for feature in locationList:\n",
    "                if feature.istitle() and feature not in locDict:\n",
    "                    locDict[feature] = 0\n",
    "                elif feature in locDict:\n",
    "                    locDict[feature] += 1\n",
    "\n",
    "        if len(userList) > 0:\n",
    "            for feature in userList:\n",
    "                if feature not in userDict:\n",
    "                    userDict[feature] = 0\n",
    "                elif feature in userDict:\n",
    "                    userDict[feature] += 1\n",
    "\n",
    "    # reomve the low frequency features in the location dictionary\n",
    "    LocDelList = []\n",
    "    for item in locDict:\n",
    "        if locDict[item] <= locNumber or len(item) == 1:\n",
    "            LocDelList.append(item)\n",
    "\n",
    "    for item in LocDelList:\n",
    "        locDict.pop(item)\n",
    "\n",
    "    for item in locDict:\n",
    "        locDict[item] = 0\n",
    "\n",
    "    # reomve the low frequency features in the user dictionary\n",
    "    UserdelList = []\n",
    "    for item in userDict:\n",
    "        if userDict[item] <= userNumber or len(item) == 1:\n",
    "            UserdelList.append(item)\n",
    "\n",
    "    for item in UserdelList:\n",
    "        userDict.pop(item)\n",
    "\n",
    "    for item in userDict:\n",
    "        userDict[item] = 0\n",
    "\n",
    "    csvfile.close()\n",
    "    return locDict, userDict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function is used to compute the number of the features in each single sample\n",
    "It will return a array that record the count of each features in all the samples(the sample's ID must in the IDlist)\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "By the way. I CANT UNDERSTAND WHY TRAIN_RAWã€‚TSV ID IS DIFFERENT FROM TRAIN_TOP100.CSV\n",
    "IT WASTE ME TOO MUCH TIME ON FINDING THE BUG\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def computeFeatures(tsv, adict, IDlist, trainset ):\n",
    "    vectors = []\n",
    "    f = open(tsv, \"r\")\n",
    "    # read each row\n",
    "\n",
    "    if trainset == True:\n",
    "        for row in f:\n",
    "            row = row.split(\"\\t\")\n",
    "            if str(row[0]) in IDlist:\n",
    "                rowDict = deepcopy(adict)\n",
    "                # row[2] is the tweet text\n",
    "                data = str(row[2])\n",
    "                data = data.replace(',', ' ').replace('/', ' ').replace('\\\\', ' ').replace(')', ' ').replace('.', ' ')\n",
    "\n",
    "                # count the number of the features and save to the dictionary\n",
    "                for word in rowDict:\n",
    "                    if word in data:\n",
    "                        rowDict[word] += 1\n",
    "                vectors.append(list(rowDict.values()))\n",
    "\n",
    "        return np.array(vectors)\n",
    "\n",
    "    else:\n",
    "        for row in f:\n",
    "            row = row.split(\"\\t\")\n",
    "            if str(row[0][1:]) in IDlist:\n",
    "                rowDict = deepcopy(adict)\n",
    "                # row[2] is the tweet text\n",
    "                data = str(row[2])\n",
    "                data = data.replace(',', ' ').replace('/', ' ').replace('\\\\', ' ').replace(')', ' ').replace('.', ' ')\n",
    "\n",
    "                # count the number of the features and save to the dictionary\n",
    "                for word in rowDict:\n",
    "                    if word in data:\n",
    "                        rowDict[word] += 1\n",
    "                vectors.append(list(rowDict.values()))\n",
    "\n",
    "        return np.array(vectors)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function is used to import the preprocessed top100 features that provided by the assignment\n",
    "It will return a array that contain these features and a label list and a Id list\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def importFeatures(data):\n",
    "    vectors = []\n",
    "    label = []\n",
    "    idList = []\n",
    "    csvfile = open(data, \"r\")\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        vectors.append(row[1:-2])\n",
    "        label.append(str(row[-1]))\n",
    "        idList.append(str(row[0][1:]))\n",
    "\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(len(vectors[i])):\n",
    "            vectors[i][j] = float(vectors[i][j])\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "    return np.array(vectors), label, idList\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This is the function is a different import. It will output (all-0 data and their labels) and (non-all-0 data and their labels)\n",
    "output 1.a array of the vectors of the features of the non-0 data 2.a list of label of these non-0 data 3.ID of these non-0 data\n",
    "4.ID of the all-0 data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def seperateImport(data):\n",
    "    vectors = []\n",
    "    label=[]\n",
    "    idList = []\n",
    "    emptyID = []\n",
    "    emptyLabel = []\n",
    "    csvfile = open(data, \"r\")\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        counter = 2\n",
    "        for i in range(len(row)):\n",
    "            if counter == len(row) - 1:\n",
    "                emptyLabel.append(str(row[-1]))\n",
    "                emptyID.append(str(row[0][1:]))\n",
    "                break\n",
    "\n",
    "            elif row[counter] != row[counter - 1]:\n",
    "                vectors.append(row[1:-2])\n",
    "                idList.append(str(row[0][1:]))\n",
    "                label.append(str(row[-1]))\n",
    "                break\n",
    "\n",
    "            elif row[counter] == row[counter - 1]:\n",
    "                counter += 1\n",
    "\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(len(vectors[i])):\n",
    "            vectors[i][j] = float(vectors[i][j])\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "    return np.array(vectors), idList,label, emptyID, emptyLabel\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    To make it easy to distinguish, In this part.I will call the non-all-0-data as part A data \n",
    "    all-0-data as part B data\n",
    "    \n",
    "    \n",
    "    For the non-all-0 data.Since the preprocessed vectors is already very useful to predict them. I will just add a small\n",
    "    amount of the location features in the existing data to make a high correction rate\n",
    "    \n",
    "    For the all-0 data.Since the preprocessed vectors are all 0. So I will use extra features to predict them.\n",
    "    These features include: location features and @username features\n",
    "    \n",
    "                                                                  \n",
    "    \"\"\"\n",
    "\n",
    "    # initialize different classifier\n",
    "    output = []\n",
    "    svc = SVC()\n",
    "    knn = KNeighborsClassifier()\n",
    "    nb = GaussianNB()\n",
    "    sgd = SGDClassifier()\n",
    "    RF = RandomForestClassifier(n_estimators=300, n_jobs=-1, oob_score=True)\n",
    "    DTC = DecisionTreeClassifier()\n",
    "    NNMLP = MLPClassifier()\n",
    "    ABC = AdaBoostClassifier()\n",
    "    QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "    \"\"\"\n",
    "    Import training and testing data\n",
    "    \"\"\"\n",
    "    givenVectors, trainLabel, trainID = importFeatures(\"train-top100.csv\")\n",
    "    testA_vectors, testA_id, testA_label, test_emptyID, test_emptyLabel = seperateImport(\"test-top100.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    The prediction for part A Data\n",
    "    \"\"\"\n",
    "    # create location dictionary, 9999 means I dont want userDict,45 means I only need the location that appear > 45\n",
    "    locDict, _ = featureEng(\"train-raw.tsv\", 40, 9999)\n",
    "\n",
    "    # mix given vectors and self-generate vectors -training\n",
    "\n",
    "    selfVectors = computeFeatures(\"train-raw.tsv\", locDict, trainID, True)\n",
    "    finalVectors_A = np.hstack((givenVectors, selfVectors))\n",
    "\n",
    "    #compute features in test set and mix with given vectors\n",
    "    selfVectors_testA = computeFeatures(\"test-raw.tsv\", locDict, testA_id, False )\n",
    "    finalVectors_testA = np.hstack((testA_vectors, selfVectors_testA))\n",
    "\n",
    "\n",
    "    # standardlize vectors\n",
    "    scaler = preprocessing.StandardScaler().fit(finalVectors_A)\n",
    "    scaler.transform(finalVectors_A)\n",
    "    scaler.transform(finalVectors_testA)\n",
    "\n",
    "    DTC.fit(finalVectors_A, trainLabel)\n",
    "    DTC_pred = DTC.predict(finalVectors_testA)\n",
    "    #print(accuracy_score(DTC_pred, testA_label))\n",
    "    # write the part A result in to output list\n",
    "    for i in range (len(DTC_pred)):\n",
    "        output.append([testA_id[i],str(DTC_pred[i])])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The prediction for part B Data\n",
    "    \"\"\"\n",
    "    #re-generate the dictionary. Reason: the all-0 data has very few meaningful data, thus we need more features to predict\n",
    "    locDict, userDict = featureEng(\"train-raw.tsv\", 0, 20)\n",
    "\n",
    "    #compute the features for the training set\n",
    "    locVectors = computeFeatures(\"train-raw.tsv\", locDict, trainID,True)\n",
    "    userVectors = computeFeatures(\"train-raw.tsv\", userDict, trainID,True)\n",
    "    finalVectors_B = np.hstack((locVectors, userVectors))\n",
    "\n",
    "    # compute features in test set and mix with given vectors\n",
    "    locVectors_testB = computeFeatures(\"test-raw.tsv\", locDict, test_emptyID,False)\n",
    "    userVectors_testB = computeFeatures(\"test-raw.tsv\", userDict, test_emptyID,False)\n",
    "    finalVectors_testB = np.hstack((locVectors_testB, userVectors_testB))\n",
    "\n",
    "    # standardlize vectors\n",
    "    scaler = preprocessing.StandardScaler().fit(finalVectors_B)\n",
    "    scaler.transform(finalVectors_B)\n",
    "    scaler.transform(finalVectors_testB)\n",
    "\n",
    "    DTC = DecisionTreeClassifier()\n",
    "    DTC.fit(finalVectors_B, trainLabel)\n",
    "    DTC_pred = DTC.predict(finalVectors_testB)\n",
    "    #print(accuracy_score(DTC_pred, test_emptyLabel))\n",
    "\n",
    "    for i in range (len(DTC_pred)):\n",
    "        output.append([test_emptyID[i],str(DTC_pred[i])])\n",
    "\n",
    "    output.sort(key=takeFirst)\n",
    "    for item in output:\n",
    "        item[0]='3'+item[0]\n",
    "\n",
    "\n",
    "    writeCSV(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
